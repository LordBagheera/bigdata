version: "3"

services:
  nifi:
    image: apache/nifi:latest
    container_name: nifi
    volumes:
      - ./shared:/mnt/shared
      - ./nificonf:/config
    ports:
      - 8443:8443
    environment:
      - "SINGLE_USER_CREDENTIALS_USERNAME=root"
      - "SINGLE_USER_CREDENTIALS_PASSWORD=ThisIsUnS3cur3P4ssw0rd"
    networks:
      - main

  spark-client:
    build:
      context: ./spark-client
      dockerfile: Dockerfile
    image: lordbagheera/spark-client:3.3.0-hadoop3
    container_name: edge-1
    volumes:
      - ./shared:/mnt/shared
    ports:
      - 2222:2222
      - 4040-4100:4040-4100
    environment:
      - TZ=America/Santiago
      - ROOT_PASSWORD=defaults
      - SPARK_APPLICATION_ARGS=spark.eventLog.enabled=true:spark.eventLog.dir=hdfs://namenode:8020/shared/spark-logs
    env_file:
      - ./hadoop.env
    networks:
      - main

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    volumes:
      - ./namenode:/hadoop/dfs/name
      - ./shared:/mnt/shared
    environment:
      - CLUSTER_NAME=bigdata
    env_file:
      - ./hadoop.env
    ports:
      - 9870:9870
    networks:
      - main

  datanode-1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode-1
    depends_on: 
      - namenode
    volumes:
      - ./datanode-1:/hadoop/dfs/data
      - ./shared:/mnt/shared
    env_file:
      - ./hadoop.env
    ports:
      - 9864:9864
    networks:
      - main  

  datanode-2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode-2
    depends_on: 
      - namenode
    volumes:
      - ./datanode-2:/hadoop/dfs/data
      - ./shared:/mnt/shared
    env_file:
      - ./hadoop.env
    ports:
      - 9865:9864
    networks:
      - main  

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop3.2.1-java8
    container_name: resourcemanager
    ports:
      - 8088:8088
    depends_on: 
      - namenode
    env_file:
      - ./hadoop.env
    volumes:
      - ./core-site.xml:/etc/hadoop/core-site.xml
      - ./yarn-site.xml:/etc/hadoop/yarn-site.xml
      - ./hdfs-site.xml:/etc/hadoop/hdfs-site.xml
      #- ./yarn-site-2.xml:/etc/hadoop/yarn-site-2.xml
      - ./mapred-site.xml:/etc/hadoop/mapred-site.xml
    networks:
      - main

  nodemanager:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop3.2.1-java8
    container_name: nodemanager
    depends_on: 
      - resourcemanager
    env_file:
      - ./hadoop.env
    deploy:
      mode: global
    labels:
       traefik.docker.network: hbase
       traefik.port: 8042
    ports:
      - 8042:8042
    networks:
      - main

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop3.2.1-java8
    container_name: historyserver
    env_file:
      - ./hadoop.env
    volumes:
      - ./core-site.xml:/etc/hadoop/core-site.xml
      - ./yarn-site.xml:/etc/hadoop/yarn-site.xml
      - ./hdfs-site.xml:/etc/hadoop/hdfs-site.xml
      #- ./yarn-site-2.xml:/etc/hadoop/yarn-site-2.xml
      - ./mapred-site.xml:/etc/hadoop/mapred-site.xml
    ports:
      - 8188:8188
    networks:
      - main

  db:
    image: mysql
    container_name: mysql
    volumes:
      - ./shared:/mnt/shared
    ports:
      - "3306:3306"
    environment:
     - MYSQL_ROOT_PASSWORD=defaults
     - MYSQL_ROOT_HOST=%
    networks:
      - main

  phpmyadmin:
    image: phpmyadmin
    container_name: phpmyadmin
    volumes:
      - ./shared:/mnt/shared
    ports:
      - "8888:80"
    environment:
      - "PMA_ARBITRARY=1"
    networks:
      - main
 
  ftp-server:
    image: delfer/alpine-ftp-server
    container_name: bigdata-ftp
    volumes:
      - ./shared:/mnt/shared
      - ./shared/ftp:/ftp/ftp
    ports:
      - "21:21"
      - "21000-21010:21000-21010"
    environment:
      - "USERS=user|defaults"
    networks:
      - main

networks:
  main:
    driver: bridge

## ELIMINO CLUSTER EXCLUSIVO DE SPARK - GENERAREMOS CLUSTER HADOOP
####  spark-master:
####    image: bde2020/spark-master:3.3.0-hadoop3.3
####    container_name: spark-master
####    volumes:
####      - ./shared:/mnt/shared
####    ports:
####      - 8080:8080
####      - 7077:7077
####    environment:
####      - INIT_DAEMON_STEP=setup_spark
####      - SPARK_APPLICATION_ARGS=spark.eventLog.enabled=true:spark.eventLog.dir=hdfs://namenode:8020/shared/spark-logs
####    env_file:
####      - ./hadoop.env
####    networks:
####      - main
####      
####  spark-worker-1:
####    image: bde2020/spark-worker:3.3.0-hadoop3.3
####    container_name: spark-worker-1
####    volumes:
####      - ./shared:/mnt/shared
####    depends_on:
####      - spark-master
####    ports:
####      - "8081:8081"
####    environment:
####      - "SPARK_MASTER=spark://spark-master:7077"
####    env_file:
####      - ./hadoop.env
####    networks:
####      - main
####
####  spark-worker-2:
####    image: bde2020/spark-worker:3.3.0-hadoop3.3
####    container_name: spark-worker-2
####    volumes:
####      - ./shared:/mnt/shared
####    depends_on:
####      - spark-master
####    ports:
####      - "8082:8081"
####    environment:
####      - "SPARK_MASTER=spark://spark-master:7077"
####    networks:
####      - main
####
####  spark-history:
####      build:
####          context: ./spark-history
####          dockerfile: Dockerfile
####      image: lordbagheera/spark-history-server:3.3.0-hadoop3.3
####      container_name: spark-history-server
####      depends_on:
####        - spark-master
####      ports:
####        - "18081:18081"
####      env_file:
####        - ./hadoop.env
####      volumes:
####        - ./shared:/mnt/shared
####      networks:
####        - main
##